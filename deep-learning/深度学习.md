---
title: 深度学习
date: 2025-07-08 10:23:54
tags: [AI, deep learning]
comment: true
mathjax: true
---

# 深度学习

## 神经网络 

### 神经元

一个接收若干信息，并产生若干结果的一个基本单位。

### 神经网络定义

神经网络由若干层构成，每层可以包含若干节点，同时从上一层接收信息，并将结果传输给下一层

### 全连接神经网络

对于朴素的神经网络，需要去考虑每一层怎么设计，每个神经元需要哪些信息，并产生什么样的结果，这样子从网络搭建上来说是很困难的。

因此，可以不用限制神经元接收的数据，让每一层的神经元均连接上一层的所有神经元(或输入层节点)。

即，将输入层数据是为一个特征向量$\vec{x}$，这个向量将被传输到隐藏层的神经元，然后计算出几个激活值(会作为一个新的向量传输到下一层)，最后传输到输出层。

### 激活函数

令$a_{j}^{[l]}$表示第$l$层中第$j$个神经元计算得到的激活值。

则：$a_{j}^{[l]} = g(\vec{w}_{j}^{[l]} \cdot \vec{a}_{j}^{[l - 1]} + b_{j}^{[l]})$

**Tips:** 怎么得到这个式子的呢，可以从朴素的逻辑回归模型来看，即输入若干特征，经过运算后得到预测结果，那么此时输入特征是哪里来的呢？必然是从上一层传输过来的，而模型的参数$(\vec{w}_{j}, b_{j})$则必然是本层的，因此只有向量$\vec{a_j}$的上标是$[l - 1]$，参数$\vec{w_{j}}, b_{j}$的上标均为$[l]$，而每一层的$a_j(j = 1, 2, \ldots, n)$将会组成一个向量$\vec{a_j}$作为输入传入下一层。

### 前向传播

根据神经网络从输入层一层一层计算最后到输出层得到结果的过程就叫做前向传播。

#### tensorflow代码实现

如下图，传入向量$\vec{x}$，经过两层运算后得到结果$\vec{a}^{[2]}$

![前向传播例1](https://github.com/1830125598/DeepLearning/raw/6c394c6adeec51039fb6a2be9e863838462ca719/deep-learning/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD_1.png)

```python
import tensorflow
import numpy as np
# 以下程序仅为网络计算流程
x = np.array([[200.0, 17.0]])
layer_1 = Dense(units = 3, activation = "sigmoid")
a1 = layer_1(x)

layer_2 = Dense(units = 1, activation = "sigmoid")
a2 = layer_2(a1)
```

#### 其他tensorflow方法

```python
# 将层按顺序堆叠，即不需要顺序编写代码进行每层运算
model = Sequential([layer_1, layer_2])
# 配置训练参数：optimizer：优化器，loss：损失函数，metrices：评估指标
model.compile(...)
# 训练模型：x：输入数据，y：训练标签，epochs：训练轮次，batch_size:批次大小
model.fit(x, y)
# 使用训练好的模型对新数据进行预测
model.predict(x_new)
```

#### 前向传播朴素实现

此时依然以上图模型为例：

```python
x = np.array([200, 17])
# 依次计算第一层3个神经元的激活值
w1_1 = np.array([1, 2])
b1_1 = np.array([-1])
z1_1 = np.dot(w1_1, x) + b1_1
a1_1 = sigmoid(z1_1)

w1_2 = np.array([-3, 4])
b1_2 = np.array([1])
z1_2 = np.dot(w1_2, x) + b1_2
a1_2 = sigmoid(z1_2)

w1_3 = np.array([-3, 4])
b1_3 = np.array([1])
z1_3 = np.dot(w1_3, x) + b1_3
a1_3 = sigmoid(z1_3)
# 合并成向量传输给下一层
a1 = np.array([a1_1, a1_2, a1_3])

w2_1 = np.array([-7, 8, 3])
b2_1 = np.array([3])
z2_1 = np.dot(w2_1, a1) + b2_1
a2_1 = sigmoid(z2_1)
```

#### 前向传播一般实现

##### 一般形式

```python
#传入上一层激活值，神经元两种参数，以及激活函数，返回下一层激活值
def dense(a_in, W, b, g): 
    units = W.shape[1]
    a_out = np.zeros(units)
    for j in range(units):
        w = W[:, j]
        z = np.dot(w, a_in) + b[j]
        a_out[j] = g(z)
    return a_out

# 前向传播
def sequential(x):
    a1 = dense(x, W1, b1)
    a2 = dense(a1, W2, b2)
    a3 = dense(a2, W3, b3)
    a4 = dense(a3, W4, b4)
    f_x = a4
    return f_x
```

##### numpy优化

```python
# 注意，此时A_in，W，B均为大写，表明这是个矩阵，而非向量
def dense(A_in, W, B, g):
    # 使用numpy中矩阵乘法函数优化程序
    Z = np.matual(A_in, W) + B
    A_out = g(Z)
    return A_out
```

### TensorFlow

#### 基础代码流程

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
# 步骤1:设计模型
model = Sequential([
    Dense(units = 25, activation='sigmoid'),
    Dense(units = 15, activation='sigmoid'),
    Dense(units = 1, activation='sigmoid'),
])
#步骤2:使用指定的损失函数编译模型
from tensorflow.keras.losses import BinaryCrossentropy
model.compile(loss=BinaryCrossentropy())
#步骤3:训练模型(fit函数实现了前向传播，损失计算反向传播，迭代的过程)
model.fit(X, Y, epochs = 100)
```

### 激活函数

#### Sigmoid

* $\sigma(x) = \frac{1}{1 + e^{-x}}$

**优点：** 

1. 输出有明确的概率含义，是的作为二分类问题的输出层
2. 连续性好，适合求导

**缺点：**

1. `梯度消失问题`：当输入值过大或过小时，函数斜率接近 $0$，导致反向传播时梯度几乎为 $0$，网络难以训练（深层网络中尤为明显）。
2. 输出均值不是 $0$（偏向 $0.5$），会导致下一层输入的分布偏移，影响训练效率。

**适用场景**

主要用于二分类问题的输出层，隐藏层中已较少使用（被 ReLU 替代）。

#### ReLU(Rectified Linear Unit，修正线性单元)

* $ReLU(x) = max(0, x)$

**优点：**

1. 解决梯度消失：$x \gt 0$ 时斜率恒为 $1$，反向传播时梯度不衰减，深层网络能有效训练（这是它成为主流的核心原因）。
2. 计算速度快（无需指数运算，比 `sigmoid` 高效）。
3. 引入 `稀疏激活`：负数输入直接输出 $0$，相当于部分神经元 “休眠”，减少冗余计算，模拟生物神经元的特性。

**缺点：**

1. `死亡 ReLU 问题`：当输入长期为负数时，神经元会永久 “休眠”（输出恒为 $0$，梯度也为 $0$，无法更新权重）。
2. 输出范围不固定（$x$ 可无限大），可能导致训练不稳定（可通过批归一化缓解）。

**适用场景：**

几乎是隐藏层的默认选择，尤其在深层网络（如 CNN、MLP）中广泛使用。

#### Linear(线性激活函数)

* $g(z) = z$

此时可以视为无激活函数。

**适用场景：** 

当预测值即可能是正数，也可能是负数时，选择线性激活函数。

### 多类

多分类问题仍然是个分类问题，此时$y$仍然只能取少量的几个值，而不是任意值。

#### Softmax

以下为4分类情况下`Softmax`回归公式。

![Softmax_例1](https://github.com/1830125598/DeepLearning/raw/a900e2b94333a1b928ca57697d7b9ea52be792a3/deep-learning/Softmax.png)

而对于一般情况，则对于$y = 1, 2, 3, \ldots, N$，有：

* $z_j = \vec{w}_{j} \cdot \vec{x} + b_{j} \quad j = 1, \ldots, N$

* $a_j = \frac{e^{z_{j}}}{\sum\limits_{k = 1}^{N}e^{z_{k}}} = P(yy = j | \vec{x})$

**note:** $a_1 + a_2 + \ldots + a_N = 1$

#### Softmax的代价函数

**代价函数**

$$
\begin{aligned}
& a_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2} + \ldots + e^{z_N}} = P(y = 1 | \vec{x}) \\\\
& \vdots \\\\
& a_N = \frac{e^{z_N}}{e^{z_1} + e^{z_2} + \ldots + e^{z_N}} = P(y = N | \vec{x})
\end{aligned}
$$

**损失函数**

$$
loss(a_1, \ldots, a_N, y) = \begin{cases}
    -log(a_1) & \text{if } y = 1 \\\\
    -log(a_2) & \text{if } y = 2 \\\\
    \vdots \\\\
    -log(a_N) & \text{if } y = N
\end{cases}
$$


#### tensorflow实现

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
# 这里将输出层改为softmax
model = Sequential([
    Dense(units = 25, activation='relu'),
    Dense(units = 15, activation='relu'),
    Dense(units = 10, activation='softmax'),
])
# 这里使用稀疏交叉熵函数
from tensorflow.keras.losses import SparseCategoricalCrossentropy
model.compile(loss=SparseCategoricalCrossentropy())
#步骤3:训练模型
model.fit(X, Y, epochs = 100)
```

**优化**

```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model = Sequential([
    Dense(units = 25, activation='relu'),
    Dense(units = 15, activation='relu'),
    Dense(units = 10, activation='linear'),
])
from tensorflow.keras.losses import SparseCategoricalCrossentropy
# 将输出层设置为仅使用线性激活函数，并同时设置放置激活函数，
# 以及将交叉熵损失添加到损失函数规范中
# 此时系统会自动调用softmax计算结果，因此输出层只需要输出原始数据即可
model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))
model.fit(X, Y, epochs = 100)
```

**Tips:** `logits`，即未经过激活函数处理的原始分数，而不是概率值

### 高级优化方法

#### Adam algorithm(Adaptive Moment estimation)

* 如果参数在梯度下降过程中一致在接近的方向上移动，那么就增大学习率。

* 如果参数在梯度下降过程中来回震荡，那么就减小学习率。

#### 使用Adam的代码

```python
model = Sequential([
    tf.keras.layers.Dense(units = 25, activation='sigmoid'),
    tf.keras.layers.Dense(units = 15, activation='sigmoid'),
    tf.keras.layers.Dense(units = 10, activation='linear'),
])

model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

model.fit(X, Y, epochs = 100)
```

### 卷积神经网络

#### 卷积层(Convolutional Layer)

任何神经元只读取前一层输入信息的一部分。

**作用：** 

* 加快计算速度* 对训练集的需求更小

如下图：第一层包含$9$个神经元，每个神经元只能读取20个特征，第二层
包含$3$个神经元，每个神经元可以读取5个特征。

 ![](https://github.com/1830125598/DeepLearning/raw/f23a666a8d140240a57168da4c6d255460536a4d/deep-learning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)
 
### 如何构建机器学习系统

#### 调试方法

![](https://github.com/1830125598/DeepLearning/raw/91dec4c5e64967c914c45bc658633e678608e8c1/deep-learning/%E8%B0%83%E8%AF%95%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F.png)

如上图，已经实现了正则化线性回归来预测房价，但是该模型在预测中出现了巨大的问题，因此需要按以下步骤进行调试：

1. 获取更多训练集
2. 尝试较小的特征集
3. 尝试更多的特征集
4. 尝试增加更多的多项式特征$(x_1^{2}, x_{2}^{2}, x_{1} x_{2}, etc)$
5. 尝试减少$\lambda$
6. 尝试增加$\lambda$

**hint：** 1，2，6主要用于解决高方差(过拟合)；3，4，5主要用于解决高偏差(欠拟合)

#### 模型评估方法

**一、通过以下函数计算的结果可以判断模型是否优秀**

**线性回归问题**

1. 训练模型使用的代价函数 $J(\vec{w}, b)$

* $J(\vec{w}, b) = \mathop{min}\limits_{\vec{w}, b}[\frac{1}{2m_{train}}\sum\limits_{i = 1}^{m_{train}}(f_{\vec{w}, b}(\vec{x}^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m_{train}}\sum\limits_{j = 1}^{n}w_{j}^{2}]$

2. 计算测试集误差

* $J_{test}(\vec{w}, b) = \frac{1}{2m_{test}}[\sum\limits_{i = 1}^{m_{test}}(f_{\vec{w}, b}(\vec{x}_{test}^{(i)}) - y_{test}^{(i)})^{2}]$

3. 计算训练集误差

* $J_{train}(\vec{w}, b) = \frac{1}{2m_{train}}[\sum\limits_{i = 1}^{m_{train}}(f_{\vec{w}, b}(\vec{x}_{train}^{(i)}) - y_{train}^{(i)})^{2}]$

**分类问题**

1. 代价函数 $J(\vec{w}, b)$

* $J(\vec{w}, b) = -\frac{1}{m} \sum\limits_{i = 1}^{m}[y^{(i)}log(f_{\vec{w}, b}(\vec{x}^{(i)})) + (1 - y^{(i)})log(1 - f_{\vec{w}, b}(\vec{x}^{(i)}))] + \frac{\lambda}{2m}\sum\limits_{j = 1}^{n}w_{j}^{2}$

2. 计算测试集误差

* $J_{test}(\vec{w}, b) = -\frac{1}{m_{test}} \sum\limits_{i = 1}^{m_{test}}[y_{test}^{(i)}log(f_{\vec{w}, b}(\vec{x}_{test}^{(i)})) + (1 - y_{test}^{(i)})log(1 - f_{\vec{w}, b}(\vec{x}_{test}^{(i)}))]$

3. 计算训练集误差

* $J_{train}(\vec{w}, b) = -\frac{1}{m_{train}} \sum\limits_{i = 1}^{m_{train}}[y_{train}^{(i)}log(f_{\vec{w}, b}(\vec{x}_{train}^{(i)})) + (1 - y_{train}^{(i)})log(1 - f_{\vec{w}, b}(\vec{x}_{train}^{(i)}))]$

### 模型选择

#### 交叉验证集

在不接触测试集的前提下，为模型优化提供 “客观反馈”，确保模型不仅能拟合训练数据，还能适应未见过的数据（即提升泛化能力）。

**即：** 不在将数据集仅划分为训练集，测试集两部分，而是将数据集划分为训练集，交叉验证集，测试集三部分。

#### 三个数据集的误差

`训练集误差：`

* $J_{train}(\vec{w}, b) = \frac{1}{2m_{train}}[\sum\limits_{i = 1}^{m_{train}}(f_{\vec{w}, b}(\vec{x}^{(i)}) - y^{(i)})^{2}]$

`交叉验证集误差：`

* $J_{cv}(\vec{w}, b) = \frac{1}{2m_{cv}}[\sum\limits_{i = 1}^{m_{cv}}(f_{\vec{w}, b}(\vec{x}_{cv}^{(i)}) - y_{cv}^{(i)})^{2}]$

`测试集误差：`

* $J_{test}(\vec{w}, b) = \frac{1}{2m_{test}}[\sum\limits_{i = 1}^{m_{test}}(f_{\vec{w}, b}(\vec{x}_{test}^{(i)}) - y_{test}^{(i)})^{2}]$

**Tips：** 训练集误差用于训练模型，交叉验证集误差用于选择训练好的若干模型，而测试集误差用于检查模型泛化能力。

#### 模型选择过程

1. 依次计算出各个模型在交叉验证集上的误差。

2. 选择交叉验证集误差最小的模型。

3. 使用测试集检查模型泛化能力。

#### 诊断偏差和方差

如下图所示：

* 出现高偏差的情况下，往往模型存在欠拟合的问题。
* 出现高方差的情况下，往往存在过拟合的问题。
* 高偏差和高方差如果同时存在，则此时模型对于任何数据表现都是很差的。

![](https://github.com/1830125598/DeepLearning/raw/5bdc4c3e117e47eae4c6fd4fe185aa6d926046da/deep-learning/%E5%81%8F%E5%B7%AE%3A%E6%96%B9%E5%B7%AE.png)

**Hint：**

* 如果模型出现高偏差，那么仅仅增加训练集是无法改善误差率，需要对模型进行调整。
* 如果模型出现高方差，那么增加训练集往往可以使得模型更加拟合。

#### $\lambda$的选择

对于正则化参数$\lambda$的选择，可以通过多次尝试的方式，找到模型表现最好的$\lambda$值。

**例：**

依次尝试以下$\lambda$值:

* $\text{Try } \lambda = 0$
* $\text{Try } \lambda = 0.01$
* $\text{Try } \lambda = 0.02$
* $\text{Try } \lambda = 0.04$
* $\text{Try } \lambda = 0.08$
* $\vdots$
* $\text{Try } \lambda = 10$

找到交叉验证误差最小的一个$\lambda$值

#### 确立表现的基准水平

为了评估模型的误差是否高时，往往需要建立一个基准水平用于比较。

以下是几种常见的基准水平取值来源：

1. 人类的表现能力
2. 竞争者的算法表现(比如以前的算法，竞争对手的算法等)
3. 根据以往的经验进行猜测

#### 学习曲线

根据模型训练的过程绘制出以下学习曲线。

观察下图可以发现：

 经过越多的数据训练，交叉验证集的误差是原来越小的。

而训练集的误差反而是从$0$开始不断增加的，这看起来很反常识，但实际上，对于越小的训练集，想要拟合是越容易的，此时预测的误差也会比较小(实际上就是欠拟合)，而随着训练数据的增加，尽管训练集误差在不管增加，但实际上模型的泛化能力在不断加强。

![](https://github.com/1830125598/DeepLearning/raw/43f280a43af5fbb9ae66c7b9cf9f3508b7ed883f/deep-learning/%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png)

#### 神经网络模型调试流程

![](https://github.com/1830125598/DeepLearning/raw/16dcc08a363add0ea76c87dcaea88e903de24778/deep-learning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E8%AF%95%E6%B5%81%E7%A8%8B.png)

如上图所示：

1. 在搭建模型时，先检查训练好的模型在训练集上的误差，如果对比基准水平偏差更高，那么可以尝试更大的神经网络，比如增加隐藏层的层数，或者每层增加隐藏单元。
2. 当模型在训练集上的误差已经接近基准水平后，继续检查模型在交叉验证集上的表现，如果表现不好，那么增加训练集的数量继续训练模型。
3. 增加数据集训练后回到第$1$步。
4. 当交叉验证集的误差也接近基准水平后，那么就表示当前模型已经训练完成。

#### 模型正则化

未正则化的`MNIST`模型(手写数字识别模型)

```pyhon
layer_1 = Dense(units = 25, activation='relu')
layer_2 = Dense(units = 15, activation='relu')
layer_3 = Dense(units = 1, activation='sigmoid')
model = Sequential([layer_1, layer_2, layer_3])
```

正则化的`MNIST`模型

```pyhon
layer_1 = Dense(units = 25, activation='relu', kernel_regularizer = L2(0.01))
layer_2 = Dense(units = 15, activation='relu', kernel_regularizer = L2(0.01))
layer_3 = Dense(units = 1, activation='sigmoid', kernel_regularizer = L2(0.01))
model = Sequential([layer_1, layer_2, layer_3])

```

**注：** 

1. 如果对大模型进行合适的正则化，那么大模型可以得到和小模型一样，或是更好的表现，而不是出现过拟合。
2. `kernel_regularizer = L2(0.01)`的含义是为该层设置一个$\lambda = 0.01$

### 开发机器学习系统的过程

#### 机器学习开发的迭代循环

机器学习系统的开发往往是按以下步骤不断循环迭代：

1. 决定系统架构（模型，数据，以及其他）
2. 训练模型
3. 实现和查看一些诊断根据

![](https://github.com/1830125598/DeepLearning/raw/6e081ca449567ece1866af2b6db0d88524597bfd/deep-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BE%AA%E7%8E%AF%E8%BF%AD%E4%BB%A3.png)